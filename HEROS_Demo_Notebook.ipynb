{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4250a8c3",
   "metadata": {},
   "source": [
    "# HEROS: Demonstration Notebook\n",
    "This notebook is set up as a demonstration for running the HEROS algorithm on different example datasets (with a binary outcome). This major sections of this notebook includes the following:\n",
    "* Notebook Run Parameters\n",
    "* Package Imports and Folder Management\n",
    "* Example Data Setup\n",
    "* Load and Prepare Training Dataset (including expert knowledge score generation - optionally used by HEROS)\n",
    "* Run HEROS (Training)\n",
    "* Testing Data Evaluation (focused on the top model from the model Pareto-front automatically chosen by the recommended strategy)\n",
    "* Visualize Top Model (i.e. Rule-Set) For Interetation\n",
    "* Calculating Top Model Feature Importance Estimates\n",
    "* Example Prediction Reasoning Explanation (With Top Model)\n",
    "* Visualize Rule and Model Population Pareto Fronts\n",
    "* Saving Rule and Model Populations as Output\n",
    "* Save and Visualize Learning Performance across Phase I and Phase 2 Training Iterations\n",
    "* Saving Other Outputs\n",
    "* Visual Interpretation and Predictions With the Whole Phase I Rule Population\n",
    "* Testing Evaluation with Top Default Model or Custom Selected Model\n",
    "* Evaluation of Stored Rule Populations (At User-Specified Iteration Checkpoints)\n",
    "* Evaluation of Stored Top Model (At User-Specified Iteration Checkpoints)\n",
    "\n",
    "## Example Dataset Options\n",
    "### **MUX6: (6-bit Multiplexer Dataset)**\n",
    "Prior to 90/10 partitioning (into train/test sets), included 500 instances (not all unique) and 6 total features (all predictive) with no noise (i.e. an trained model is capable of predicting with 100% accuracy). Features A_0 and A_1 are 'address-bits' that are predictive for every instance. Features R_0, R_1, R_2, and R_3 are 'register-bits' that are predictive for about 1/4 of all instances, each. All features and outcome (i.e. 'class') are binary-valued, and there are no missing values. MUX datasets involved feature interactions and heterogeneous patters of association. \n",
    "### **MUX11: (11-bit Multiplexer Dataset)**\n",
    "Prior to 90/10 partitioning, included 5000 instances and 11 total features (all predictive) with no noise. Features A_0, A_1, and A_2 are 'address-bits', while the rest are register bits.\n",
    "### **MUX20: (20-bit Multiplexer Dataset)**\n",
    "Prior to 90/10 partitioning, included 10000 instances and 20 total features (all predictive) with no noise. Features A_0, A_1, A_2, and A_3 are 'address-bits', while the rest are register bits.\n",
    "### **GAM_A: (GAMETES Simulated Dataset A - 4 Additive Univariate Features)**\n",
    "Prior to 90/10 partitioning, included 1600 instances and 100 total features (4 are predictive - M0P0, M1P0, M2P0, M3P0) with a small degree of overall simulated noise. Each feature has a univariate association with outcome, with the strongest association being an additive combination of all 4 predictive features. Outcome (i.e. 'Class') is binary-valued.\n",
    "### **GAM_C: (GAMETES Simulated Dataset C - 2-way Epistatic Interaction)**\n",
    "Prior to 90/10 partitioning, included 1600 instances and 100 total features (2 are predictive - M0P0, M0P1) with about 60% simulated noise. Both predictive features have low/no univariate association with outcome. Outcome (i.e. 'Class') is binary-valued.\n",
    "### **GAM_E: (GAMETES Simulated Dataset E - 4 Heterogeneous Univariate Features)**\n",
    "Prior to 90/10 partitioning, included 1600 instances and 100 total features (2 are predictive - M0P0, M0P1) with about 60% simulated noise. Both predictive features have low/no univariate association with outcome. Outcome (i.e. 'Class') is binary-valued."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b62610",
   "metadata": {},
   "source": [
    "***\n",
    "## Notebook Run Parameters\n",
    "The parameters below control basic notebook functionality and allow users to change other HEROS run parameters for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acff4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Operation Parameters ---------------------------------------------------------------------------------\n",
    "load_from_cloned_repo = False # Leave True if running from cloned repository, and change to False if relying on 'pip' installation of HEROS\n",
    "local_save = True # If True, saves output in pre-designated local folder within cloned repository folder\n",
    "folder_path = './output' # Specify new path for output files generated by this notebook (if local_save = False)\n",
    "run_all_cells = True # If True, run all cells of notebook (i.e. generate all example outputs and visualizations). False will only run key cells\n",
    "\n",
    "# Specify which example dataset to run --------------------------------------------------------------------------\n",
    "example_dataset = 'MUX6' # Dataset Options: 'MUX6', 'MUX11', 'MUX20', 'GAM_A', 'GAM_C', 'GAM_E'\n",
    "\n",
    "# Expert Knowldge Generation Run Parameters ---------------------------------------------------------------------\n",
    "max_instances = 2000 # Maximum number of available training instances to use in estimating feature importance scores with 'MultiSURF' algorithm.\n",
    "use_turf = False # Idicate whether to use TuRF wrapper algorithm in combination with MultiSURF (recommended for large feature spaces, e.g. > 10000 features)\n",
    "turf_pct = 0.2 # Controls the number of TuRF iterations as well as the number of features removed from calculations each iteration. (0.2 runs for 5 iterations with 20% of bottom scoring features removed each time)\n",
    "\n",
    "# HEROS Key Hyperparameters -------------------------------------------------------------------------------------\n",
    "iterations = 50000 # (Rule_I) Number of Phase I learning iterations (e.g. 50000)\n",
    "pop_size = 500 # (Rule_P) Maximum size of Phase I rule population (micro-population size) (e.g. 500)\n",
    "model_iterations = 100 # (Model_I) Number of Phase II learning iterations (e.g. 100)\n",
    "model_pop_size = 100 # (Model_P) Maximum size of Phase II model population (e.g. 100)\n",
    "nu = 1 # (v) Accuracy pressure (1 is neutral)\n",
    "\n",
    "# Other HEROS Hyperparameters -----------------------------------------------------------------------------------\n",
    "beta = 0.2 # (B) Average {M} size learning rate\n",
    "theta_sel = 0.5 # (0_sel) {M} proportion for Phase I tournament selection\n",
    "cross_prob = 0.8 # (p_cross) probability of applying crossover (Phase I and II)\n",
    "mut_prob = 0.04 # (p_mut) probability of applying mutation (Phase I and II)\n",
    "merge_prob = 0.1 # (p_merge) probability of applying merge (Phase II)\n",
    "new_gen = 1.0 # Controls size of {OMP} as function of Model_P\n",
    "model_pop_init = 'target_acc' # Model initialization method ('random' or 'target_acc')\n",
    "subsumption = 'both' # Use GA, {C} or 'both' subsumption mechanisms ('ga', or 'c', 'both', or None)\n",
    "rsl = 0 # Manually specify rule specificity limit (Give positive integer or leave 0 for automatic rule specificity limit determination from dataset properties)\n",
    "compaction = 'sub' # rule compaction method ('sub' or None)\n",
    "random_state = 42 # Applied random seed (for reproducibility)\n",
    "\n",
    "# HEROS Performance tracking hyperparameters --------------------------------------------------------------------\n",
    "track_performance = 1000 # Number of Phase I iterations where performance metrics are periodically estimated\n",
    "model_tracking = True # Track top model performance across training iterations\n",
    "stored_rule_iterations = '500,1000,5000,10000,50000' # Specified Phase I iterations where the current rule {P} is archived for external evaluation (e.g. '500,1000,5000,10000' or None)\n",
    "stored_model_iterations = '10,50,100' # Specified Phase II iterations where the current {MP} is archived for external evaluation (e.g. '10,50,100', or None)\n",
    "verbose = True # Run in 'verbose' mode - display run details\n",
    "\n",
    "# In-Development HEROS Hyperparameters (Recommend not changing) -------------------------------------------------\n",
    "outcome_type = 'class' # Only 'class' (i.e. classification outcome) is operational in current implementation\n",
    "fitness_function = 'pareto' # Fitness function for Phase I rule discovery ('accuracy' or 'pareto') Pareto is strongly recommended unless analyzing clean problems.\n",
    "feat_track = 'end' # Feature tracking strategy applied (None, 'add', 'wh', 'end') Experimental - recommended to leave to None\n",
    "rule_pop_init = None # Specifies rule population pre-initialization method (None, 'load', or 'dt') Experimental - recommended to leave to None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd8d78",
   "metadata": {},
   "source": [
    "***\n",
    "## Package Imports & Folder Management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f931305f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skheros'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mskheros\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mheros\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HEROS\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Load HEROS Notebook via pip installation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskheros\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mheros\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HEROS\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load all other packages used in this notebook --------------------------------\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skheros'"
     ]
    }
   ],
   "source": [
    "# Load heros algorithm ---------------------------------------------------------\n",
    "if load_from_cloned_repo: # Load HEROS algorithm locally from cloned github repository\n",
    "    from src.skheros.heros import HEROS\n",
    "else: # Load HEROS Notebook via pip installation\n",
    "    from skheros.heros import HEROS\n",
    "\n",
    "# Load all other packages used in this notebook --------------------------------\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skrebate import MultiSURF, TURF # Install using: pip install skrebate==0.7\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Determine notebook output folder path ------------------------------------------------------\n",
    "if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "if local_save:\n",
    "    output_path = './output'\n",
    "else:\n",
    "    output_path = folder_path\n",
    "\n",
    "# Report current working directory ---------------------------------------------\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76d492",
   "metadata": {},
   "source": [
    "***\n",
    "## Example Data Setup\n",
    "Sets up the various options for each possible example dataset (as selected above by the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dataset parameters\n",
    "train_data_path = None\n",
    "test_data_path = None\n",
    "outcome_label = 'Class' # Class/outcome column label in target dataset\n",
    "instanceID_label = 'InstanceID' #None for datasets without an instance ID column\n",
    "excluded_column = None\n",
    "\n",
    "if example_dataset == 'MUX6': # 6-bit Multiplexer\n",
    "    train_data_path = 'evaluation/datasets/partitioned/multiplexer/A_multiplexer_6_bit_500_inst_CV_Train_1.txt'\n",
    "    test_data_path = 'evaluation/datasets/partitioned/multiplexer/A_multiplexer_6_bit_500_inst_CV_Test_1.txt'\n",
    "    excluded_column = 'Group'\n",
    "elif example_dataset == 'MUX11': # 11-bit Multiplexer\n",
    "    train_data_path = 'evaluation/datasets/partitioned/multiplexer/B_multiplexer_11_bit_5000_inst_CV_Train_1.txt'\n",
    "    test_data_path = 'evaluation/datasets/partitioned/multiplexer/B_multiplexer_11_bit_5000_inst_CV_Test_1.txt'\n",
    "    excluded_column = 'Group'\n",
    "elif example_dataset == 'MUX20': # 20-bit Multiplexer\n",
    "    train_data_path = 'evaluation/datasets/partitioned/multiplexer/C_multiplexer_20_bit_10000_inst_CV_Train_1.txt'\n",
    "    test_data_path = 'evaluation/datasets/partitioned/multiplexer/C_multiplexer_20_bit_10000_inst_CV_Test_1.txt'\n",
    "    excluded_column = 'Group'\n",
    "elif example_dataset == 'GAM_A': # GAMETES Dataset A (additive)\n",
    "    train_data_path = 'evaluation/datasets/partitioned/gametes/A_uni_4add_CV_Train_1.txt'\n",
    "    test_data_path = 'evaluation/datasets/partitioned/gametes/A_uni_4add_CV_Test_1.txt'\n",
    "elif example_dataset == 'GAM_C': # GAMETES Dataset C (epistasis)\n",
    "    train_data_path = 'evaluation/datasets/partitioned/gametes/C_2way_epistasis_CV_Train_1.txt'\n",
    "    test_data_path = 'evaluation/datasets/partitioned/gametes/C_2way_epistasis_CV_Test_1.txt'\n",
    "elif example_dataset == 'GAM_E': # GAMETES Dataset E (heterogeneous)\n",
    "    train_data_path = 'evaluation/datasets/partitioned/gametes/E_uni_4het_CV_Train_1.txt'\n",
    "    test_data_path = 'evaluation/datasets/partitioned/gametes/E_uni_4het_CV_Test_1.txt'\n",
    "    excluded_column = 'Model'\n",
    "else:\n",
    "    print(\"Specified Example Dataset Not Found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762d17f",
   "metadata": {},
   "source": [
    "***\n",
    "## Load and Prepare Training Dataset\n",
    "Data used for training HEROS has the following requirements:\n",
    "1. The training data must be passed as separate array-like objects including:\n",
    "    1. 'X' {n_samples, n_features} Training instance features.\n",
    "    3. 'y' {n_samples} Training labels of the outcome variable.\n",
    "3. 'y must always be provided and must not include any missing values.\n",
    "4. Missing values are allowed in 'X' but data instances should be excluded that have missing values for all features. Note, HEROS treats missing values as missing, i.e. no imputation of values are made or needed. Any rule that specifies a given feature will not match an instance with a missing value at that instance. \n",
    "\n",
    "Required data preparation here includes separating the loaded dataset into X and y array-like objects, and removing unnecessary columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffac0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset ------------------------\n",
    "train_df = pd.read_csv(train_data_path, sep=\"\\t\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Create unique output folder for specific target dataset ------------------------\n",
    "file_name = os.path.splitext(os.path.basename(train_data_path))[0]\n",
    "output_path = output_path+'/'+file_name\n",
    "if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "#Prepare Training Data ------------------------\n",
    "try:\n",
    "    X = train_df.drop(excluded_column, axis=1) #Remove excluded column from consideration in this notebook\n",
    "except:\n",
    "    X = train_df\n",
    "    print('Excluded column not available')\n",
    "try:\n",
    "    X = X.drop(instanceID_label,axis=1)\n",
    "except:\n",
    "    print('Instance ID coulmn not available')\n",
    "\n",
    "X = X.drop(outcome_label, axis=1)\n",
    "feature_names = X.columns \n",
    "cat_feat_indexes = list(range(X.shape[1])) #all feature are treated as categorical so provide indexes 0-5 in this list for 6-bit multiplexer dataset e.g. [0,1,2,3,4,5]\n",
    "\n",
    "#Finalize separate array-like objects for X and y\n",
    "X = X.values\n",
    "y = train_df[outcome_label].values #outcome values\n",
    "try:\n",
    "    row_id = train_df[instanceID_label].values #instance id values\n",
    "except:\n",
    "    row_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458f639",
   "metadata": {},
   "source": [
    "### Generate Expert Knowlege Scores for Features (Optional, but suggested in HEROS) \n",
    "Previous LCS research has indicated that using Relief-based algorithms such as 'MultiSURF' to generate feature importance scores as a guide for rule initialization improves evolutionary algorithm performance. Here we show how these scores can be generated, saved, and later used by HEROS for rule initialization. \n",
    "\n",
    "These scores only need to be generated once for a given training dataset and then can be saved for future use. This code only runs MultiSURF if the scores have not yet been generated for a training dataset with a unique name.\n",
    "\n",
    "The MultiSURF (feature importance estimation) algorithm is found in our scikit-rebate respository at https://github.com/UrbsLab/scikit-rebate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_sampling(df,outcome_label, max_instances):\n",
    "    \"\"\" Returns a dataframe with a sampled number of rows from the original (retaining class balance if possible).\n",
    "        Assumes that outcome values are either 0 or 1. \"\"\"\n",
    "    # Split the DataFrame by class\n",
    "    df_class_0 = df[df[outcome_label] == 0]\n",
    "    df_class_1 = df[df[outcome_label] == 1]\n",
    "    # Determine the number of rows to sample per class\n",
    "    n_class_0 = max_instances // 2\n",
    "    n_class_1 = max_instances - n_class_0  # Assign remaining rows to Class 1\n",
    "    # Ensure we don't sample more rows than available\n",
    "    n_class_0 = min(n_class_0, len(df_class_0))\n",
    "    n_class_1 = min(n_class_1, len(df_class_1))\n",
    "    # Sample rows from each class\n",
    "    sampled_class_0 = df_class_0.sample(n=n_class_0, replace=False)\n",
    "    sampled_class_1 = df_class_1.sample(n=n_class_1, replace=False)\n",
    "    # Combine the sampled rows\n",
    "    sampled_df = pd.concat([sampled_class_0, sampled_class_1]).reset_index(drop=True)\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further data preparation for expert knowldge generation (i.e. balanced subsampling of training instances for faster run times)\n",
    "fs_train_df = balanced_sampling(train_df, outcome_label, max_instances)\n",
    "try:\n",
    "    fs_X = fs_train_df.drop(excluded_column, axis=1) #Remove excluded column from consideration in this notebook\n",
    "except:\n",
    "    fs_X = fs_train_df\n",
    "    print('Excluded column not available')\n",
    "try:\n",
    "    fs_X = fs_X.drop(instanceID_label,axis=1)\n",
    "except:\n",
    "    print('Instance ID coulmn not available')\n",
    "fs_X = fs_X.drop(outcome_label, axis=1)\n",
    "#Finalize separate array-like objects for X and y\n",
    "fs_X = fs_X.values\n",
    "fs_y = fs_train_df[outcome_label].values #outcome values\n",
    "#Optional TURF Parameters --------------\n",
    "num_scores_to_return = int(X.shape[1]/2.0)\n",
    "# --------------------------------------\n",
    "score_path_name = None\n",
    "if use_turf:\n",
    "    score_path_name = output_path+'/MultiSURF_TuRF_Scores.csv' #No need to change\n",
    "else:\n",
    "    score_path_name = output_path+'/MultiSURF_Scores.csv' #No need to change\n",
    "# Calculate or load feature importance estimates with MultiSURF or MultiSURF+TuRF ------------------------------------\n",
    "if not os.path.isfile(score_path_name):\n",
    "    if use_turf: # Run MultiSURF with TuRF wrapper\n",
    "        print(\"Generating MultiSURF/TuRF Scores:\")\n",
    "        clf = TURF(MultiSURF(n_jobs=None), pct=turf_pct, num_scores_to_return=num_scores_to_return).fit(fs_X, fs_y)\n",
    "        ek = clf.feature_importances_\n",
    "        score_data = pd.DataFrame({'Feature':feature_names,'Score':ek})\n",
    "        score_data.to_csv(score_path_name,index=False)\n",
    "    else: #Just run MultiSURF\n",
    "        print(\"Generating MultiSURF Scores:\")\n",
    "        clf = MultiSURF(n_jobs=None).fit(fs_X, fs_y)\n",
    "        ek = clf.feature_importances_\n",
    "        score_data = pd.DataFrame({'Feature':feature_names,'Score':ek})\n",
    "        score_data.to_csv(score_path_name,index=False)\n",
    "else: #load previously trained scores\n",
    "    print(\"Loading MultiSURF Scores\")\n",
    "    loaded_data = pd.read_csv(score_path_name)\n",
    "    ek = loaded_data['Score'].tolist()\n",
    "print(list(feature_names))\n",
    "print(ek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5058a",
   "metadata": {},
   "source": [
    "***\n",
    "## Run HEROS (Training)\n",
    "\n",
    "1. (OPTIONAL) 'pop_df' is a DataFrame of a previously trained HEROS rule population that can be uploaded to initialize a new HEROS run.\n",
    "2. (OPTIONAL) 'ek' is a numpy array or list of expert knowledge weights for all quantitative and categorical features in the dataset. They must be formatted as follows:\n",
    "    1. This list must have the same number of elements as there are total features in the dataset (ordered in the same way)\n",
    "    2. If a mix of quantiative and categorical features are loaded, HEROS will combine them with all quantitative features first, followed by all categorical features in the same order as provided in 'X' or 'Xc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42047be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HEROS algorithm with run parameters\n",
    "heros = HEROS(outcome_type=outcome_type,iterations=iterations,pop_size=pop_size,cross_prob=cross_prob,mut_prob=mut_prob,nu=nu,beta=beta,theta_sel=theta_sel,\n",
    "              fitness_function=fitness_function,subsumption=subsumption,rsl=rsl,feat_track=feat_track, model_iterations=model_iterations,\n",
    "              model_pop_size=model_pop_size,model_pop_init=model_pop_init,new_gen=new_gen,merge_prob=merge_prob,rule_pop_init=rule_pop_init,compaction=compaction,\n",
    "              track_performance=track_performance,model_tracking=model_tracking,stored_rule_iterations=stored_rule_iterations,stored_model_iterations=stored_model_iterations,random_state=random_state, verbose=verbose)\n",
    "\n",
    "heros = heros.fit(X, y, row_id, cat_feat_indexes=cat_feat_indexes, ek=ek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98270d45",
   "metadata": {},
   "source": [
    "***\n",
    "## Testing Data Evaluation\n",
    "### Load and Prepare Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing dataset ---------------------------\n",
    "test_df = pd.read_csv(test_data_path, sep=\"\\t\")\n",
    "print(test_df.head())\n",
    "#Prepare Testing Data ----------------------------\n",
    "try:\n",
    "    X_test = test_df.drop(excluded_column, axis=1)\n",
    "except:\n",
    "    X_test = test_df\n",
    "try:\n",
    "    X_test = X_test.drop(instanceID_label, axis=1)\n",
    "except:\n",
    "    pass\n",
    "X_test = X_test.drop(outcome_label, axis=1)\n",
    "#Finalize separate array-like objects for X and y\n",
    "X_test = X_test.values\n",
    "y_test = test_df[outcome_label].values #outcome values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f82b86",
   "metadata": {},
   "source": [
    "### Identify Top Model from the Model Pareto Front\n",
    "Evaluates the testing data performance of all non-dominated models on the top-ranking Phase II Pareto front, each treated as a candidate solution. Stepping through all non-dominated models, we replace the current best if the next model either (1) has > balanced testing accuracy and >= testing coverage, or (2) has >= balanced testing accuracy, >= testing coverage, and a smaller rule count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47927583",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = heros.auto_select_top_model(X_test,y_test,verbose=True)\n",
    "set_df = heros.get_model_rules(best_model_index)\n",
    "print(set_df) #Print all rules of the top model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e136f4",
   "metadata": {},
   "source": [
    "### Evaluate Top Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report performance results for the top model\n",
    "predictions = heros.predict(X_test,whole_rule_pop=False, target_model=best_model_index)\n",
    "print(\"HEROS Top Model Testing Data Performance Report:\")\n",
    "print(classification_report(predictions, y_test, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf21232",
   "metadata": {},
   "source": [
    "### Get Prediction Probabilities and Coverage Confirmation for all Testing Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_prob = heros.predict_proba(X_test,whole_rule_pop=False, target_model=best_model_index)\n",
    "print(\"Prediction Probabilities for all Testing Instances:\")\n",
    "print(predict_prob)\n",
    "\n",
    "predict_cover = heros.predict_covered(X_test,whole_rule_pop=False, target_model=best_model_index)\n",
    "print(\"Coverage for all Testing Instances:\")\n",
    "print(predict_cover)\n",
    "print(str(sum(predict_cover))+' instances covered out of '+str(len(predict_cover)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df757f01",
   "metadata": {},
   "source": [
    "### ROC Plot of Top Model Testing Performance\n",
    "Set up for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79beb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predict_prob[:, 1]) #based on class 1\n",
    "# Calculate AUC\n",
    "roc_auc = roc_auc_score(y_test, predict_prob[:, 1]) #based on class 1\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")  # diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165430ea",
   "metadata": {},
   "source": [
    "### Save Rules of Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_df = heros.get_model_rules(best_model_index)\n",
    "set_df.to_csv(output_path+'/top_testing_model_rules.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2250c",
   "metadata": {},
   "source": [
    "***\n",
    "## Visualize Top Model (i.e. Rule-Set) For Interetation\n",
    "### Specified Features Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells:\n",
    "    heros.get_rule_set_heatmap(feature_names, best_model_index, weighting='useful_accuracy', specified_filter=1, display_micro=False, show=True, save=True, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960939dc",
   "metadata": {},
   "source": [
    "### Feature Specification as Network with Feature Co-Occurence as Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells:\n",
    "    node_size = 1000\n",
    "    edge_size = 10\n",
    "    heros.get_rule_set_network(feature_names, best_model_index, weighting='useful_accuracy', display_micro=False, node_size=node_size, edge_size=edge_size, show=True, save=True, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c2c9c",
   "metadata": {},
   "source": [
    "### Feature Tracking Scores (Across Training Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3950089",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells and feat_track != None:\n",
    "    heros.run_model_feature_tracking(best_model_index)\n",
    "    # Save Feature Tracking Scores to .csv\n",
    "    ft_df = heros.get_model_ft(feature_names)\n",
    "    ft_df.shape\n",
    "    ft_df.to_csv(output_path+'/rule_set_'+str(best_model_index)+'_feature_tracking_scores.csv', index=False)\n",
    "    # Visualize feature tracking scores as clustered heatmap\n",
    "    heros.get_clustered_model_ft_heatmap(feature_names, specified_filter=1, show=True, save=True, output_path=output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28fb04",
   "metadata": {},
   "source": [
    "***\n",
    "## Calculating Top Model Feature Importance Estimates\n",
    "Using scikit-learn's permutation importance package function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a336b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run permutation importance\n",
    "result = permutation_importance(heros, X_test, y_test, n_repeats=100, random_state=random_state, scoring='balanced_accuracy')\n",
    "# Extract importance means\n",
    "importances = result.importances_mean\n",
    "std = result.importances_std\n",
    "# Sort features by importance\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "# Generate Simple Feature Importance Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names[sorted_idx], importances[sorted_idx], xerr=std[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance (Mean Decrease in Balanced Accuracy)\")\n",
    "plt.title(\"Estimated Model Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Most important on top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3917ed",
   "metadata": {},
   "source": [
    "***\n",
    "## Example Prediction Reasoning Explanation (With Top Model)\n",
    "When applying the trained models to unlabeled data for prediction, the cells in this section give a basic example of how the prediction reasoning may be explained in clear human interpretable terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66567604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get example testing instance (with no label) --------------------------------\n",
    "print(\"Total Number of Available Testing Instances: \"+str(len(X_test)))\n",
    "target_testing_instance_index = 0 #arbitrarily chosen as an example\n",
    "target_testing_instance = X_test[target_testing_instance_index]\n",
    "print(\"Making Prediction on Testing Instance Index: \"+str(target_testing_instance_index))\n",
    "# Apply prediction to target instance -----------------------------------------\n",
    "heros.predict_explanation(target_testing_instance, feature_names, whole_rule_pop=False, target_model=best_model_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4b37e",
   "metadata": {},
   "source": [
    "***\n",
    "## Visualize Rule and Model Population Pareto Fronts\n",
    "### **Rule Population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb557984",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells:\n",
    "    resolution = 500\n",
    "    plot_rules = True\n",
    "    color_rules = True\n",
    "    heros.get_rule_pareto_landscape(resolution, heros.rule_population, plot_rules, color_rules,show=True,save=True,output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef354b",
   "metadata": {},
   "source": [
    "### **Model Population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells:\n",
    "    resolution = 500\n",
    "    plot_models = True\n",
    "    heros.get_model_pareto_fronts(show=True,save=True,output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d7219",
   "metadata": {},
   "source": [
    "***\n",
    "## Saving Rule and Model Populations as Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf56653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Rule Population as .csv\n",
    "rule_pop_df = heros.get_pop()\n",
    "rule_pop_df.to_csv(output_path+'/rule_pop.csv', index=False)\n",
    "# Save Model Population as .csv\n",
    "model_pop_df = heros.get_model_pop()\n",
    "model_pop_df.to_csv(output_path+'/model_pop.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e5690",
   "metadata": {},
   "source": [
    "***\n",
    "## Save and Visualize Learning Performance across Phase I and Phase 2 Training Iterations\n",
    "### **Phase I (Rule Learning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb01410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Phase 1 Rule Training Performance Estimates to .csv\n",
    "rule_tracking_df = heros.get_performance_tracking()\n",
    "rule_tracking_df.to_csv(output_path+'/rule_pop_tracking.csv', index=False)\n",
    "# Plot Rule Learning Tracking\n",
    "heros.get_rule_tracking_plot(show=True,save=True,output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cd576",
   "metadata": {},
   "source": [
    "### **Phase II (Model Learning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tracking_df = heros.get_model_performance_tracking()\n",
    "model_tracking_df.to_csv(output_path+'/model_tracking.csv', index=False)\n",
    "# Plot Model Learning Tracking\n",
    "heros.get_model_tracking_plot(show=True,save=True,output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f857822a",
   "metadata": {},
   "source": [
    "***\n",
    "## Saving Other Outputs\n",
    "### Pickle Trained HEROS Object (For Future Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle HEROS object\n",
    "with open(output_path+'/heros.pickle', 'wb') as f:\n",
    "    pickle.dump(heros, f)\n",
    "# Load previously pickled HEROS Object\n",
    "with open(output_path+'/heros.pickle', 'rb') as f:\n",
    "    heros = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac4187",
   "metadata": {},
   "source": [
    "### Document HEROS Run Paramter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heros.save_run_params(output_path+'/heros_run_parameters.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b866c33",
   "metadata": {},
   "source": [
    "### Save Run Time Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = heros.get_runtimes()\n",
    "time_df.to_csv(output_path+'/runtimes.csv', index=False)\n",
    "print(time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d83875",
   "metadata": {},
   "source": [
    "***\n",
    "## Visual Interpretation and Predictions With the Whole Phase I Rule Population\n",
    "### Vizualize Rule Population (Feature Specification) as a Rule-Clustered Heatmap (With Optional Rule-Weighting)\n",
    "Parameters:\n",
    "* *feature_names*: a list of feature names for the entire training dataset (given in original dataset order)\n",
    "*  *weighting*: indicates what (if any) weighting is applied to individual rules for the plot ('useful_accuracy', 'fitness', None)\n",
    "*  *specified_filter*: the number of times a given feature must be specified in rules of the population to be included in the plot (must be a positive integer or None)\n",
    "*  *display_micro*: controls whether or not additional copies of rules (based on rule numerosity) should be included in the heatmap (True or False) \n",
    "*  *show*: indicates whether or not to show the plot (True or False)\n",
    "*  *save*: indicates whether or not to save the plot to a specified path/filename (True or False)\n",
    "*  *output_path*: a valid folder path within which to save the plot (str of folder path)\n",
    "*  *data_name*: a unique name precursor to give to the plot (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells:\n",
    "    heros.get_rule_pop_heatmap(feature_names, weighting='useful_accuracy', specified_filter=1, display_micro=True, show=True, save=True, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b424a",
   "metadata": {},
   "source": [
    "### Vizualize Rule Population (Feature Specification) as a Network (With Optional Rule-Weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ea1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_cells:\n",
    "    node_size = 1000\n",
    "    edge_size = 10\n",
    "    weighting = 'useful_accuracy'# 'useful_accuracy', 'fitness', None\n",
    "    display_micro = True\n",
    "    heros.get_rule_pop_network(feature_names, weighting=weighting, display_micro=display_micro, node_size=node_size, edge_size=edge_size, show=True, save=True, output_path=output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a1dfc",
   "metadata": {},
   "source": [
    "### Vizualize Rule Population Feature Tracking Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Feature Tracking Scores to .csv\n",
    "if heros.feat_track != None:\n",
    "    ft_df = heros.get_ft(feature_names)\n",
    "    ft_df.shape\n",
    "    ft_df.to_csv(output_path+'/feature_tracking_scores.csv', index=False)\n",
    "# Visualize clustered heatmap of feature tracking scores across all training instances\n",
    "if heros.feat_track != None and run_all_cells:\n",
    "    heros.get_clustered_ft_heatmap(feature_names, show=True, save=True, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845cd32",
   "metadata": {},
   "source": [
    "### Prediction with Whole Phase I Rule Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a269b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = heros.predict(X_test,whole_rule_pop=True)\n",
    "print(\"HEROS Whole Rule Population Testing Data Performance Report:\")\n",
    "print(classification_report(predictions, y_test, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce7ad8",
   "metadata": {},
   "source": [
    "***\n",
    "## Testing Evaluation with Top Default Model or Custom Selected Model\n",
    "### **Top Default Model** (i.e. selected as model on front with highest training accuracy)\n",
    "Ranked by accuracy, then coverage, then rule-count (as tie-breakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4390561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Top Model selected by Default from the Front (Model on front with highest training accuracy)\n",
    "set_df = heros.get_model_rules() #returns top training model by default based on balanced accuracy, then covering, then rule-set size.\n",
    "set_df.to_csv(output_path+'/top_default_model_rules.csv', index=False)\n",
    "print(set_df)\n",
    "# Model Predictions and Evaluation using this 'Default' top model\n",
    "predictions = heros.predict(X_test)\n",
    "print(\"HEROS Top 'Default' Model Testing Data Performance Report:\")\n",
    "print(classification_report(predictions, y_test, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832bfd8",
   "metadata": {},
   "source": [
    "### **Custom Selected Model** (Any Model in Trained Model Population)\n",
    "The model index represents the index of the model in the final trained model population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeab87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Top Model selected by Default from the Front (Model on front with highest training accuracy)\n",
    "desired_model_index = 1\n",
    "set_df = heros.get_model_rules(index=desired_model_index) #returns top training model by default based on balanced accuracy, then covering, then rule-set size.\n",
    "print(set_df)\n",
    "# Model Predictions and Evaluation using this 'Default' top model\n",
    "predictions = heros.predict(X_test,whole_rule_pop=False,target_model=desired_model_index)\n",
    "print(\"HEROS Top 'Default' Model Testing Data Performance Report:\")\n",
    "print(classification_report(predictions, y_test, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47df5e",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation of Stored Rule Populations (At User-Specified Iteration Checkpoints)\n",
    "To facilitate comparing algorithm performance at earlier learning iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stored_rule_iterations != None:\n",
    "    rule_iteration_list = [int(x) for x in stored_rule_iterations.split(',')]\n",
    "    for iterations in rule_iteration_list:\n",
    "        print(\"Rule population evaluation at iteration \"+str(iterations))\n",
    "        print(\"Run Time: \"+str(heros.timer.rule_time_archive[iterations]))\n",
    "        predictions = heros.predict(X_test,whole_rule_pop=True,rule_pop_iter=iterations)\n",
    "        print(classification_report(predictions, y_test, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6efcf",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation of Stored Top Model (At User-Specified Iteration Checkpoints)\n",
    "To facilitate comparing algorithm performance at earlier learning iterations.\n",
    "### **With Default Top Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stored_model_iterations != None:\n",
    "    model_iteration_list = [int(x) for x in stored_model_iterations.split(',')]\n",
    "    for iterations in model_iteration_list:\n",
    "        print(\"Top default model evaluation at iteration \"+str(iterations))\n",
    "        print(\"Run Time: \"+str(heros.timer.model_time_archive[iterations]))\n",
    "        predictions = heros.predict(X_test,whole_rule_pop=False,model_pop_iter=iterations)\n",
    "        print(classification_report(predictions, y_test, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a183f3",
   "metadata": {},
   "source": [
    "### **With Top Model Selected From Pareto-Front Using Testing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stored_model_iterations != None:\n",
    "    model_iteration_list = [int(x) for x in stored_model_iterations.split(',')]\n",
    "    for iterations in model_iteration_list:\n",
    "        print('---------------------------------------------------------------------------------------------')\n",
    "        print(\"Top model evaluation at iteration \"+str(iterations))\n",
    "        print(\"Run Time: \"+str(heros.timer.model_time_archive[iterations]))\n",
    "        iter_best_model_index = heros.auto_select_top_model(X_test,y_test,verbose=True,model_pop_iter=iterations)\n",
    "        predictions = heros.predict(X_test,whole_rule_pop=False, target_model=iter_best_model_index)\n",
    "        print(classification_report(predictions, y_test, digits=8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
